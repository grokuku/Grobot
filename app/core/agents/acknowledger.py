import logging
from typing import List

from app.core.agents.prompts import ACKNOWLEDGER_SYSTEM_PROMPT
from app.core.llm.ollama_client import get_llm_response
from app.schemas.chat_schemas import ChatMessage, PlannerResult

logger = logging.getLogger(__name__)


def _summarize_plan_for_prompt(plan: PlannerResult) -> str:
    """
    Creates a brief, human-readable summary of the execution plan.

    Args:
        plan: The execution plan generated by the Planner.

    Returns:
        A string summarizing the plan's main actions.
    """
    if not plan.plan:
        return "do nothing."

    # Extracts the names of the tools to be called.
    tool_names = [step.tool_name for step in plan.plan]
    
    if len(tool_names) == 1:
        return f"use the '{tool_names[0]}' tool."
    else:
        # e.g., "execute the following tools in order: get_time, get_weather, generate_image."
        return f"execute the following tools in order: {', '.join(tool_names)}."


async def run_acknowledger(
    plan: PlannerResult,
    bot_personality: str,
    bot_name: str,
    history: List[ChatMessage]
) -> str:
    """
    Runs the Acknowledger agent to create a user-facing waiting message.

    Args:
        plan: The execution plan, to give the agent context on the task.
        bot_personality: The personality instructions for the bot.
        bot_name: The name of the bot.
        history: The conversation history.

    Returns:
        A short, personality-consistent message to inform the user.
    """
    logger.debug("Running Acknowledger...")

    plan_summary = _summarize_plan_for_prompt(plan)

    # 1. Inject dynamic values into the prompt template.
    prompt = ACKNOWLEDGER_SYSTEM_PROMPT.strip()
    prompt = prompt.replace("{{bot_name}}", bot_name)
    prompt = prompt.replace("{{bot_personality}}", bot_personality)
    prompt = prompt.replace("{{execution_plan_summary}}", plan_summary)

    messages = [msg.model_dump() for msg in history]

    try:
        # 2. Call the LLM for a simple text response.
        response = await get_llm_response(
            system_prompt=prompt,
            messages=messages
        )
        
        ack_message = response.strip()
        logger.info(f"Acknowledger generated message: '{ack_message}'")
        return ack_message

    except Exception as e:
        logger.error(f"An unexpected error occurred in Acknowledger: {e}", exc_info=True)
        # Provide a safe, generic fallback message.
        return "Got it, I'm working on your request."